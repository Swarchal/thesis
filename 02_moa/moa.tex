\documentclass[a4paper,11pt,twoside,openright]{scrbook}

\usepackage{swThesis}
\usepackage{amsmath}
\usepackage{lipsum}
\usepackage{microtype}
\usepackage{booktabs}
\usepackage{standalone}
\usepackage{epigraph}  % For dedication and quote
\usepackage{bibentry}  % Allow full citations in text

\usepackage{import}
\standalonetrue

\bibliography{bibliography}

% Figures
\graphicspath{{../figs/}}

\begin{document}

\chapter{Cell morphology can be used to predict compound mechanism-of-action} \label{chapter:moa}

\section{Introduction}
Cellular morphology is influenced by multiple intrinsic and extrinsic factors acting on a cell, and striking changes in morphology are observed when cells are exposed to biologically active small molecules.
This compound-induced alteration in morphology is a manifestation of various perturbed cellular processes, and we can hypothesise that compounds with similar MoA which act upon the same signalling pathways will produce comparable phenotypes, and that cell morphology can, in turn, be used to predict compound MoA.

In 2010 Caie \textit{et al.} generated, as part of a larger study, an image dataset consisting of MCF7 breast cancer cells treated with 113 small molecules grouped into 12 mechanistic classes, these cells were then fixed, labelled and imaged in three fluorescent channels\cite{Caie2010}.
This dataset (also known as BBBC021) has become widely used as a benchmark in the field for MoA classification tasks, with multiple publications using the images to compare machine learning and data pre-processing approaches \cite{Ljosa2013a,Singh2014a,Pawlowski2016,Ando2017}.
Whilst this is important work, it has led to the situation whereby the vast majority of studies in this field have based their work on a single dataset generated with a one cell-line.

One of the issues associated with phenotypic screening when used in a drug discovery setting is target deconvolution.
Once a compound has been identified which results in a desirable phenotype in a disease-relevant assay it is common to want to know which molecular pathways the hit compound is acting upon.
While target deconvolution is a complex and difficult task, image-based morphological profiling represents one option similar to transcriptional profiling that can match and unknown compound to the nearest similar annotated compound in a dataset, while at the same time being far cheaper than the transcriptional methods such as LINCS1000 \cite{Duan2014}.


\section{Creating an annotated dataset}
%
As part of a preliminary study, a dataset similar to that of Caie \textit{et al.}'s was generated consisting of 24 compounds grouped into 8 mechanistic classes screened across the panel of 8 breast cancer cell-lines (see table \ref{table:cell-lines}).


% Please add the following required packages to your document preamble:
\begin{table}[]
    \begin{footnotesize}
    \centering
    \captionsetup{width=0.8\textwidth}
    \caption[Annotated compounds of known MoA]{Annotated compounds and their associated mechanism-of-action label used in the classification tasks.}
    \label{table:compounds}
    \begin{tabular}{@{}llll@{}}
    \toprule
    Compound        & MoA class              & Supplier    & Catalog no. \\ \midrule
    Paclitaxel      & Microtubule disrupting & Sigma       & T7402       \\
    Epothilone B    & Microtubule disrupting & Selleckchem & S1364       \\
    Colchicine      & Microtubule disrupting & Sigma       & C9754       \\
    Nocodazole      & Microtubule disrupting & Sigma       & M1404       \\
    Monastrol       & Microtubule disrupting & Sigma       & M1404       \\
    ARQ621          & Microtubule disrupting & Selleckchem & S7355       \\
    Barasertib      & Aurora B inhibitor     & Selleckchem & S1147       \\
    ZM447439        & Aurora B inhibitor     & Selleckchem & S1103       \\
    Cytochalasin D  & Actin disrupting       & Sigma       & C8273       \\
    Cytochalasin B  & Actin disrupting       & Sigma       & C6762       \\
    Jaskplakinolide & Actin disrupting       & Tocris      & 2792        \\
    Latrunculin B   & Actin disrupting       & Sigma       & L5288       \\
    MG132           & Protein degradation    & Selleckchem & S2619       \\
    Lactacystin     & Protein degradation    & Tocris      & 2267        \\
    ALLN            & Protein degradation    & Sigma       & A6165       \\
    ALLM            & Protein degradation    & Sigma       & A6060       \\
    Emetine         & Protein synthesis      & Sigma       & E2375       \\
    Cycloheximide   & Protein synthesis      & Sigma       & 1810        \\
    Dasatinib       & Kinase inhibitor       & Selleckchem & S1021       \\
    Saracatinib     & Kinase inhibitor       & Selleckchem & S1006       \\
    Lovastatin      & Statin                 & Sigma       & PHR1285     \\
    Simvastatin     & Statin                 & Sigma       & PHR1438     \\
    Camptothecin    & DNA damaging agent     & Selleckchem & S1288       \\
    SN38            & DNA damaging agent     & Selleckchem & S4908       \\ \bottomrule
    \end{tabular}
    \end{footnotesize}
\end{table}





\subsection{Compounds}
The 24 compounds see table \ref{table:compounds} were chosen based on previous knowledge of their biological activity and wide range of morphological responses, most of the compounds feature in Caie \textit{et al.}s original dataset.


\subsection{Cell painting: labelling cellular morphology}
In order to capture a broad view of morphological changes within a cell using fluorescent microscopy, a choice has to be made which cellular structures to label.
This choice is limited by the availablity of the fluorescent filter sets fitted to the microscope, reagent costs, and the scalability of the protocol when used in a large screen.
Fortunately, this problem was already addressed by another group who published a protocol -- named ``cell painting" -- for labelling 7 cellular structures, using 6 non-antibody stains imaged in the same 5 fluorescent channels available with our miscoscopy setup. \cite{Gustafsdottir2013, Bray2016}

% development of, and overcoming the issues in the cell-staining protocol?
The cell-painting protocol was initially optimised by Gustafsdottir \textit{et al.} for use in the U2OS oesteosarcoma cell line, and briefly tested in a few other commonly used cell-lines.
However, when tested on the panel of 8 breast cancer cell lines, the staining protocol was observed to induce morphological changes on certain cell lines, in the absence of compounds.
It was found that changing the media, and adding the MitoTracker DeepRed stain to live MDA-MB-231 cells produced a rounded morphology, which was not observed in the other cell lines.  %TODO: figure of cell painting cell-rounding in MDA-MB-231 cells.
As any morpholgical changes introduced by the staining protocol would mask those caused by small-molecules, the protocol was adapted by removing the media change step, and moving the addition of wheat germ agglutinin and MitoTracker DeepRed until after fixation.
As the cells were now fixed immediately in their existing media this prevented any alterations to the morphology and improved the wheat germ agglutinin staining, although as the MitoTracker stain relies on membrane potential in mitochondria, the selectivity of the MitoTracker stain was reduced when used on fixed cells, though it still produced selective enough labelling to capture large changes in mitochondrial morphology.

%TODO: images of cell-painting statins, channels and different cell-lines


\begin{table}[]
    \begin{footnotesize}
    \centering
    \captionsetup{width=0.8\textwidth}
    \caption[Cell painting reagents and filter wavelengths for imaging.]{Reagents used in the cell painting protocol and the excitation/emission wavelengths of the filters used in imaging. ex: excitation, em: emission}
    \label{table:staining}
    \begin{tabular}{@{}lllll@{}}
    \toprule
    Stain                                                               & Labeled Structure                                                   & \begin{tabular}[c]{@{}l@{}}Wavelength\\ (ex/em {[}nm{]})\end{tabular} & Concentration & \begin{tabular}[c]{@{}l@{}}Catalog no.;\\ Supplier\end{tabular} \\ \midrule
    Hoechst 33342                                                       & Nuclei                                                              & 387/447 $\pm 20$                                                      & 2 $\mu$g/mL   & \begin{tabular}[c]{@{}l@{}}\#H1399;\\ Mol. Probes\end{tabular}  \\
    SYTO14                                                              & Nucleoli                                                            & 531/593 $\pm 20$                                                      & 3 $\mu$M      & \begin{tabular}[c]{@{}l@{}}\#S7576;\\ Invitrogen\end{tabular}   \\
    Phalloidin 594                                                      & F-actin                                                             & 562/624 $\pm 20$                                                      & 0.85 U/mL     & \begin{tabular}[c]{@{}l@{}}\#A12381;\\ Invitrogen\end{tabular}  \\
    \begin{tabular}[c]{@{}l@{}}Wheat germ\\ agglutinin 594\end{tabular} & \begin{tabular}[c]{@{}l@{}}Golgi and \\plasma membrane\end{tabular} & 562/624 $\pm 20$                                                      & 8 $\mu$g/mL   & \begin{tabular}[c]{@{}l@{}}\#W11262;\\ Invitrogen\end{tabular}  \\
    Concanavalin A 488                                                  & \begin{tabular}[c]{@{}l@{}}Endoplasmic\\ reticulum\end{tabular}     & 462/520 $\pm 20$                                                      & 11 $\mu$/mL   & \begin{tabular}[c]{@{}l@{}}\#C11252;\\ Invitrogen\end{tabular}  \\
    \begin{tabular}[c]{@{}l@{}}MitoTracker\\ DeepRed\end{tabular}       & Mitochondria                                                        & 628/692 $\pm 20$                                                      & 0.6 $\mu$M        & \begin{tabular}[c]{@{}l@{}}\#M22426;\\ Invitrogen\end{tabular}  \\ \bottomrule
    \end{tabular}
    \end{footnotesize}
\end{table}



\section{Machine learning methods to classify compound MoA}
Predicting compound MoA from phenotypic data is a classification task.
This type of machine learning problem is well researched, and there are several models appropriate for our labelled data.
As the raw data is in the form of images, it can approached as an image classification task, a problem in the field receiving lots of attention due to recent theoretical and technological breakthroughs. %citations?
Whereas a more classical approach would be to extract morphological information from the images, generating a multivariate dataset from the images, and training a classifier on these morphological features.

To develop and validate a machine learning model the dataset has to be split into training, validation and test sets.
This is because overfitting is a common problem in machine learning, whereby the model is trained and accurately predicts labels on one dataset, but performs poorly when applied to new data on which is was not trained.
Most classifiction models will overfit to some degree, typically performing better on the training dataset than any other subsequent examples, but the challenge is to limit this overfitting, and also to ensure that the data used to report accuracy measures has not been used in any way to train or validate the model.


% comparison of machine learning and data pre-processing methods to predict MOA
% from morphological data

\subsection{Comparison between classical and deep-learning machine learning methods}
% description / background of each method
% pros / cons of each
% results
% conclusion of comparison

\subsubsection{Ensemble of decision trees trained on extracted morphological features}
A decision tree is a very simple method that can be used for both regression and classification.
The method works by repeatedly dividing the decision space using binary rules on the feature values until a terminal node containing a classification label is reached (figure \ref{figure:decision_tree}).
Simple decision trees like those shown in figure \ref{figure:decision_tree} perform relatively poorly on all but the simplest of classification problems.
However, by aggregating many decision trees and their predictions we can create more accurate and robust models in a practice known as ensemble learning. \cite{Opitz1999}
Bagging \cite{Breiman1996} and Boosting \cite{Freund1996} are two popular methods for constructing ensembles of decision trees.
As combining the output of several decision trees is useful only if there is a disagreement among them, these two methods both attempt to solve the same problem of generating a set of correct decision tress, that still disagree with one another as much as possible on incorrect predictions.


\begin{figure}
    \captionsetup{width=0.8\textwidth}
    \caption[Diagram of a simple decision tree]{\textbf{(A)} An example of a simple mock decision tree to classify compound mechanism of action based on morphological features. \textbf{(B)} Depiction of decision space as divided by the decision tree model. Shaded areas show how new input data will be classified based on the decision rules (dotted lines).}
    \input{figs/ch2DecisionTree.pdf_tex}
    \label{figure:decision_tree}
\end{figure}

% using extracted morphological features
Decision tree methods work best with multivariate tabular data, with well defined features describing each observation, this is in contrast to image data which consists of 2D arrays of pixel intensities.
Therefore, in order to train such a model, cellular morphology needs to be quantified by measuring cellular features.
This is a common task with multiple software packages available, which follow two main steps: (1) Segment objects from the background. Objects may be sub-cellular structures or whole-cell masks (2) Measure various attributes from the object, this is typically based on size, shape and intensity.
Cellprofiler \cite{Carpenter2006} was chosen primarily due to the high configurability and the permissive license enabling large-scale distributed processing on compute clusters in order to reduce the image analysis time.
The images captured on the ImageXpress were analysed using Cellprofiler, quantifying approximately 400 morphological features.
The datasets produced by the Cellprofiler analysis contained morphological measurements on an individual cell level.
Although we can train a model on single cell data we are not interesting in classifying morphologies of single cells, but rather classifying an image or a collection of images that represent a compound treatment, this therefore allows several approaches to structuring the training data:

\begin{enumerate}
    \item Train and test on median profiles.
    \item Train on single cell data, test on image or well median profiles.
    \item Train on single cell data, test on single cell data and classify the parent image as the most commonly predicted class of cell in that image.
    \item Train on median profiles of boostrapped single cell samples within an image, and test on median profiles.
\end{enumerate}


% image averages, vs single cell data and consensus classification.
    % comparison of both.


\subsubsection{Convolutional neural networks trained on pixel data}
Artificial neural networks (ANNs) are becoming increasingly common in a wide range of machine learning tasks.
Although many of the theories underpinning ANNs are decades old, \cite{Rosenblatt1958} they have only recently achieved widespread practical use due to improved methods for training \cite{Rumelhart1986} and the availability of more computing power allowing the use of more complex models.
ANNs are (very) loosely inspired by the structure of biological brains, with interconnected neurons passing signals through layers onto subsequent neurons forming a chain with the output of one neuron becoming the input for the next neuron.
In between neurons, the signals can be altered by multiplying the value by a weight ($W$), it is through adjusting these individual weights that ANNs optimise their performance for a particular task, similar to how long-term potentiation is used to strengthen synaptic connections in biological brains.
When a signal reaches a neuron, it is combined via a weighted sum with all the other inputs from other connected neurons and passed through an activation function.
This activation function -- similar to an action potential in neurons -- determines the output of the neuron for the given aggregated input, which is then passed as new inputs onto subsequent neurons and so on, however, in contrast to an all-or-nothing output of an action potential there are several types of activation functions used in ANNs, most of which have a graded output (figure \ref{figure:neuron_relu}B).

\begin{figure}
    \captionsetup{width=0.9\textwidth}
    \caption[Diagram neural network neuron and activation function.]{\textbf{(A)} A representation of a single connected neuron in an ANN, the input value to the neuron is multiplied by the weight ($W_1$), before being passed through the activation function $R$, the output of which is then multiplied by $W_2$, and passed as the input to the next neuron. \textbf{(B)} A neuron with multiple inputs and outputs, typical of those in a hidden layer. The activation function acts on the weighted sum of all inputs, and returns a single output value which is when directed to all connected neurons in the next layer. Where $W_i$ is the weight of \textsf{input}$_i$. \textbf{(C)} A common activation function also known as a rectifier, in this example a rectified linear unit (ReLU), in the inputs ($x$) is transformed and passed as output. So $f(x)$ can be viewed as the output for a given value of $x$.}
    \label{figure:neuron_relu}
\input{figs/ch2NeuralNetworkNode2.pdf_tex}
\end{figure}

The neurons in an ANN are typically arranged in several layers: an input layer; one or more hidden layers; and a final output layer (figure \ref{figure:nn_layers}).
With each layer, the network transforms the data into a new representation, through training the network these representations make the data easier to classify.
In the final layer, the data is ultimately represented in a way which makes a single output neuron activate more strongly than the other neurons in that layer, and so the data is ultimately transformed into a single value -- the index of the active neuron which corresponds to a particular class.
A new ANN is initialised with random weights, to train a neural network these weights are adjusted by feeding in labelled data and adjusting weights in order to minimise classification errors through a process known as backpropagation.\cite{Rumelhart1986}




\begin{figure}
\fcapsideleft
{
    \caption[Representation of a simple ANN]{Representation of a simple 3-layer ANN with a single fully connected hidden layer, three input neurons and three output neurons. $W$ denotes a weighted connection between an input neuron and a hidden-layer neuron, with all connections between neurons having an associated adjustable weight. A network such as this would take a vector of three numbers as input, and would be capable of predicting three classes from the output layer of three neurons depending on the activation strengths of the neurons in the final output layer.}
} {
    \input{figs/ch2NeuralNetworkLayers.pdf_tex}
    \label{figure:nn_layers}
}
\end{figure}

%convolution part of convolution neural networks
The convolution aspect of convolutional neural networks plays an important role when working with image data.
Two-dimensional convolutions are widely used in image processing -- blurring, sharpening and edge detection are all common operations which use this operation.
They work by mapping a kernel -- a smaller matrix of values -- across a larger matrix, thereby using information from a small region of pixels in their transformation of each individual pixel.
This lends itself well to ANNs, as a pixel value in isolation is less informative than a pixel value in the context of the neighbouring values.
Depending on the size and the values within the kernel, the transformations highlight different features within an image.
Two dimensional convolutions are used in ANNs by starting with many randomly initialised kernels, and updating the kernel values through training in order to best highlight features which prove useful for accurately predicting classes.
Using a single convolutional layer highlights simple features in an image such as edges and speckles, by combining several convolutional layers more complex features are highlighted through combinations of these simple features.
These convolved images are then flattened into a one-dimensional vector which is used as an input in a fully connected ANN such as that depicted in figure \ref{figure:nn_layers}.

% TODO more of a segway here, more details on using more than RGB channels.

As CNNs can be constructed with a wide variety of architectures, and the field is still rapidly developing, I remained closed to well established architectures in the literature.
However, as most images are digitially represented in three colour channels (red, green, blue (RGB)), the vast majority of CNN models are constructed in a way that input is restricted to three colour channels, therefore it is necessary to adapt these architectures to work with the differently shaped inputs and additional parameters generated by the 5 channel images generated with the ImageXpress.

% TODO: overfitting, test/val accuracy and loss averaged over a few cell-lines
% for both ResNet18 (with and without extra dropout) and alexnet
% maybe: show some intermediate layers? convolutions??

% TODO: diagram of architecture
% easy for alexnet

The images generated by the ImageXpress microscope with zero binning are 2160 by 2160 pixel tiff files, with a bit-depth of 16, whilst these image properties are common in microscopy, they are extreme for current CNN implementations.
Most image classification tasks involving CNN's use 8-bit images in the region of 300 by 300 pixels, relatively small images are used as the convolutional layers of deep CNN's generate many thousands of matrices, and using smaller input images drastically reduces the computing resources and time required to train such classifiers.

This presents the issue of how to reduce the 2160$\times$2160 images into small images, one option is to downscale the entire image using bi-linear or bi-cubic interpolation, while a second option is to chop the original image up into smaller sub-images (figure \ref{figure:image_chopping}).
Downsizing the original image by simple scaling has a few potential problems which make it unsuitable for this particular task:
    many of the finer-grained cell morphologies such as mitochondria and endoplasmic reticulum distribution will be lost due to the reduction in image resolution;
    in addition, it was found that whole well images are susceptible to over-fitting as the classfier learned biologically irrelevant features such as the locations of cells within an image, which although should be random might have some spurious association with particular class labels.
When chopping images into sub-images the most simple and commonly used method is to chop each image into an evenly spaced grid, whilst this is unbiased and easy to implement, it has the downside of potentially returning many images that do not contain any cells.
A more nuanced approach is to first detect the $x$,$y$ co-ordinates of each  in the image, and creating a 300$\times$300 bounding-box around the centre of each cell.
This method returns an image per cell, negating the issue of empty images; it does however require detecting cell locations and handling cells located next to the image border.

\begin{figure}
    \includegraphics[width=0.6\textwidth]{ch2ImageChopping}
    \captionsetup{width=0.8\textwidth}
    \caption[Down-sizing and chopping images for CNN training]{Two options for adapting large microscope images to work with the smaller input size of typical CNNs. \textbf{(A)} Full-sized images are downsized to the desired dimensions via bilinear or bicubic interpolation. \textbf{(B)} Images are chopped into smaller sub-images, cell detection can be carried out beforehand to ensure images contain at least one cell.}
    \label{figure:image_chopping}
\end{figure}


\section{Predicting compound MoA on single cell lines}
The first step is to determine a baseline of how well the predictive models perform when trained and tested on the same cell line, this also acts as a platform with which to test and optimise model architecture and hyperparameters.


The two different CNN architectures were tested based on the hypothesis that a deeper, more complex architecture (ResNet18 \cite{He2015}) will be capable of learning more subtle features, although more complex models with greater numbers of interal parameters are more prone to overfitting when training data is limited.
On the other hand, a more simple model such as AlexNet \cite{Krizhevsky2012} which contains fewer convolutional layers will be less able to perform complex tranformations of the data, and therefore theoretically limit the subtle features which can be extracted and learned from an image.
While this might theoretically reduce accuracy, in the absence of large amount of training data it may reduce overfitting due to the fewer number of parameters.
% TODO alexnet, resnet (with and without dropout) training graphs for a few cell-lines
% pick an architecture
% train, test, classify MoA for a single cell-line



\subsection{Ensemble of decision trees}
% how much of this should go in methods?
% data preparation
% which model were tested
% grid-search of parameters
% performance
% which model/parameters/data-type were settled on?


\subsection{CNNs}
% how much of this should go in methods?
% data type(s)
% models tested
% parameters trialled
% performance
% overfitting graphs
% which model/parameters were settled on?


\section{Transferring machine learning models to morphologically distinct cell lines}
TODO: How well machine learning models generalise across cell-lines.


%\begin{figure}
%    { \fontsize{4pt}{4pt}\fontfamily{lmss}\selectfont
%        \input{figs/ch2ConfusionMatricesCNN.pdf_tex}
%    }
%\end{figure}


\subsection{Leave-one-out classification}
TODO.
% train on 7 cell-lines
% test on witheld 8th cell-line

\subsection{Do additional cell-lines during training increase prediction accuracy?}
TODO: When predicting MoA for a given cell-line, does adding additional training examples from other morphologically distinct cell-lines improve classification accuracy?
% for predicting MoA in a single cell-line, does adding additional data from other cell-lines during training improve classification accuracy?


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DISCUSSION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion}
TODO.








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% METHODS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\section{Methods}

\subsection{Cell culture}
%Cells seeding densities, min-max, plate effects etc...
The cell-lines were all grown in DMEM (\#CATNO MANUFACTURER) supplemented with 10\% foetal bovine serum and 2 mM L-glutamine, incubated at 37$^\circ$C, 5\% CO$_2$.
2,500 cells were seeded into the inner 60 wells of optical bottomed 96-well plates (\#165305 ThermoFisher) in 100 $\mu$L of media, whilst outer wells were filled with 100 $\mu$L of PBS.

After seeding with cells, assay plates were incubated for 24 hours prior to the addition of compounds.


\subsection{Compound handling}
Compounds (table \ref{table:compounds}) were diluted in DMSO at a stock concentration of 10 mM.
Compounds plates were made in v-bottomed 96-well plates (\#CATNO MANUFACTURER), at 1000-fold concentration in 100\% DMSO by serial dilutions ranging from 10 mM to 0.3 $\mu$M in semi-log concentrations.
Compounds were added to assay plates containing cells after 24 hours of incubation by first making a 1:50 dilution in media to create an intermediate plate, followed by a 1:20 dilution from intermediate plate to the assay plate, with an overall dilution of 1:1000 from the stock compound plate to the assay plate.


\subsection{Staining}
After 48 hours in the presence of compounds, assay plates were fixed by adding equal volume (100 $\mu$L) of 8\% paraformaldehyde (\#CATNO MANUFACTURER) to the existing media, resulting in a final paraformaldehyde concentration of 4\%, and left to incubate at room temperature for 30 minutes.
Wells were then washed with 100 $\mu$L of PBS and permeabilised with a 0.1\% Triton-X100 solution for 20 minutes at room temperature.
A cell-staining solution was made up in 1\% bovine serum albumin (BSA) solution (see table \ref{table:staining}).
Wells were then washed again with 100 $\mu$L of PBS followed by addition of 30 $\mu$L staining solution.
Plates were then incubated in the dark for 30 minutes at room temperature, washed 3 times with 100 $\mu$L of PBS.
Before the final aspiration, plates were then sealed (\#CATNO MANUFACTURER) and imaged.


\subsection{Imaging}
Imaging was carried out on a multi-wavelength wide-field fluorescent microscope (ImageXpress micro XL, MolecularDevices, USA) with a robotic plate loader (Scara4, PAA, UK).
Images were captured in 5 fluorescent channels (see table \ref{table:staining}) at 20x magnification, exposure times were kept constant between plates and batches, as to not influence intensity values used in subsequent analyses.


\subsection{Ensemble of decision trees}
Models were created using scikit-learn version 0.19 in python 3.6.2.
% model
% parameters, grid search of parameters, table of final parameters.
% training, test, train, validation etc.

\subsection{Image analysis: numeric features from images}
TODO

\subsection{Convolutional neural networks}


All code related to neural networks was written in pytorch v0.3 for python 3.5, and all ANN models were trained on Nvidia K80 GPUs.
As training CNNs is computationally expensive and time consuming, data parallelism was leveraged to share batches of images across multiple GPUs.
This technique replicates the CNN model on each device, which processes a portion of the input data, the updated weights for all devices are then averaged and model replicates are updated synchronously after each batch (figure \ref{figure:multi_GPU}).
This speeds up model training approximately linearly with the number of GPUs and allows use of larger batch sizes.

\begin{figure}
    \captionsetup{width=0.8\textwidth}
    \caption[Multi-GPU distributed training]{Increased training speed by data parallelism. Models are replicated across an array of GPUs, the input batch is split evenly among the devices, with each device processing a portion in parallel. During backpropagation the updated weights for all replicas are averaged and models weights are updated synchronously.}
    \input{figs/ch2MultiGPU.pdf_tex}
    \label{figure:multi_GPU}
\end{figure}
% library
% architecture




% TODO: training
    % size of datasets
    % learning rate, batch_size, epochs, dropout, transformations, multi-GPU
    % training

\subsubsection{Image preparation}
TODO
% number of images
% encoding


%\printbibliography
\end{document}
